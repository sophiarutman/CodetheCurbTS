---
title: "Trey's Stat's Work"
author: Trey Roark
---

Hey queens, this is Trey's Code the Curb working file. 

# Library Loading
```{r, warning=FALSE}
suppressMessages(library(knitr)) # Table building
suppressMessages(library(kableExtra)) # Table Customization
suppressMessages(library(readr)) # Read CSV as tibbles
suppressMessages(library(dplyr)) # Data Manipulation
```



# Project Work

## Project Overview

There are a few data tools available now that can serve as examples of how visualization and data tools can improve the parking experience (Park Mobile's “Availability” layer (https://www.youtube.com/watch?v=Tiq5GfAoHrg); eXactnav app; CurbIQ website (https://www.curbiq.io/)). However, we want to know what else you can do with data to make parking better for drivers and other road users in Arlington. How else can this data be used or displayed to achieve the pilot project's goals? 


## Project Thoughts

I do think we're gonna need to brainstorm a little more than just basic visualizations after I get the data, it almost seems like they're going for a shark tank type of ordeal. Maybe we can take a data science perspective and give some trends that we see in the data provided by the API


## Data Loading

To ensure the data loading from the API is achieved efficiently, I'll be using the corresponding Python file, `data_loading.py`, to load the data from the API, and I'll use R, what I'm familiar with, for data manipulation and visualizations. 

The API (known on the website as the Arlington Performance Parking Project Real-Time Stall Status API) comes from the Performance Parking Project that made parking sensor real-time data available to developers to develop their own applications.

The website with the API says: "The response will contain the status of all stalls with a parking sensor in the parking project. There are over 4500 stalls in the parking project area. The number of sensors will vary due to other projects such as paving or construction which may remove or add sensors. At the time of release there were over 4200 sensors installed and returning data."

```{r}
# Load in the data achieved from the fetch request
parking_stall_status <- read_csv("data/parking_stall_status.csv")
```

The data is in real time, so the first **action item** we should do is to collect this data over a certain period of time so the potential for time analysis is available. 


### Data Details

The API grants us with the following information listed in the table below:

```{r}
# Create a data frame with field definitions the API data provides
field_definitions <- data.frame(
  Field = c("version", "timezone", "stallID", "stallName", 
            "blockfaceID", "status", "location", "payloadTimestamp"),
  Description = c(
    "API version number",
    "The time zone of Arlington County",
    "Unique numerical identifier",
    "Name of stall, consists of the BlockFaceID and an assigned number",
    "A unique identifier for the block face where the stall is located",
    "Status of the stall, either 'vacant' or 'occupied'",
    "Latitude and longitude of the stall",
    "The date and time in the local timezone when the status packet was sent."
  )
)

# Display the table with white text to better visualize what the API provides (table is also on the website)
kable(field_definitions, caption = '<span style="font-size: 25px; font-weight: bold;">Field Definitions for Arlington Parking API</span>') |>
  kable_styling("striped", full_width = F) |>
  row_spec(0, background = "black", color = "white") |>
  row_spec(1:nrow(field_definitions), color = "white") |>
  column_spec(1, background = "darkblue") |>
  column_spec(2, background = "darkblue")

```


Along with the API data that comes with certain fields obtained from the fetch request (in the `data_loading.py` file), they also link something connecting the data to their parking infrastructure. 

The website says "The developer may want to tie the stall status back into the County parking inventory. The parking meter data is available via the County GIS Open Data (https://gisdata-arlgis.opendata.arcgis.com). This data is free to download. The parking meter data contains meter identifiers, block number, full street, Metro Area as well as parking information such as time limits, pricing and other regulation information. The real-time data can be linked back to the meter data through the BlockFaceID. Each meter in the project area has a BlockFaceID in the meter data."

So if we wanted to work with GIS data to supplement the real time data that we get from the API we can get that from the website above


## Project Ideas

WHAT IF: We did a continuous log of the data over the week of Halloween (I made a script that runs overtime, so we can keep getting the data every 30 minutes), and did a Halloween themed presentation talking about how busy Arlington is around the holidays? 

This will allow us to take the current data that we've collected and visualize it/analyze it over time, maybe build some models or something like that. It won't stop us from visualizing one specific dataset and then exporting that to the larger dataset we collect over time.  



